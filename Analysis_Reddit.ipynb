{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring alcohol-related content on Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\"alcohol\",\n",
    "\"stopdrinking\",\n",
    "\"cripplingalcoholism\",\n",
    "\"drugscirclejerk\",\n",
    "\"Drugs\",\n",
    "\"AmItheAsshole\",\n",
    "\"beer\",\n",
    "\"unpopularopinion\",\n",
    "\"drunk\",\n",
    "\"Alcoholism_Medication\",\n",
    "\"dryalcoholics\",\n",
    "\"AlcoholGifRecipes\",\n",
    "\"alcoholism\",\n",
    "\"alcoholicsanonymous\",\n",
    "\"Sober\",\n",
    "\"ShittyLifeProTips\",\n",
    "\"AlAnon\",\n",
    "\"AskReddit\",\n",
    "\"todayilearned\",\n",
    "\"Showerthoughts\",\n",
    "\"trees\",\n",
    "\"exmormon\",\n",
    "\"teenagers\",\n",
    "\"NoStupidQuestions\",\n",
    "\"explainlikeimfive\",\n",
    "\"science\",\n",
    "\"funny\",\n",
    "\"worldnews\",\n",
    "\"Art\",\n",
    "\"keto\",\n",
    "\"news\",\n",
    "\"politics\",\n",
    "\"askscience\",\n",
    "\"memes\",\n",
    "\"pics\",\n",
    "\"SkincareAddiction\",\n",
    "\"ireland\",\n",
    "\"australia\",\n",
    "\"Marijuana\",\n",
    "\"AskHistorians\",\n",
    "\"LifeProTips\",\n",
    "\"mildlyinteresting\",\n",
    "\"india\",\n",
    "\"kratom\",\n",
    "\"Jokes\",\n",
    "\"AdviceAnimals\",\n",
    "\"Homebrewing\",\n",
    "\"conspiracy\",\n",
    "\"Health\",\n",
    "\"loseit\",\n",
    "\"canada\",\n",
    "\"alcoholic\",\n",
    "\"alcoholabuse\",\n",
    "\"alcoholfreebeer\",\n",
    "\"alcoholismprotips\",\n",
    "\"AlcoholInkArt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "#helper functions to clean up texts by eliminating stopwords and urls\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "import string\n",
    "\n",
    "STOP_PREFIXES = (\"@\", \"http\", \"&amp\")\n",
    "\n",
    "def clean_text(comment):\n",
    "    comment = comment.replace(\"\\n\", \" \")\n",
    "    words = []\n",
    "\n",
    "    for word in TweetTokenizer().tokenize(comment):\n",
    "        if word not in list(string.punctuation) and word not in stopwords.words(\"english\") and not word.startswith(STOP_PREFIXES):\n",
    "            words.append(word)\n",
    "    \n",
    "    return (\" \").join(words)\n",
    "\n",
    "#helper functions to get sentiment scores\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def find_positive(comment):\n",
    "    return sia.polarity_scores(comment)[\"pos\"]\n",
    "\n",
    "def find_neutral(comment):\n",
    "    return sia.polarity_scores(comment)[\"neu\"]\n",
    "\n",
    "def find_negative(comment):\n",
    "    return sia.polarity_scores(comment)[\"neg\"]\n",
    "\n",
    "def find_compound(comment):\n",
    "    return sia.polarity_scores(comment)[\"compound\"]\n",
    "\n",
    "#main analyses\n",
    "positive_scores = {}\n",
    "neutral_scroes = {}\n",
    "negative_scores = {}\n",
    "compound_scores = {}\n",
    "\n",
    "agg_text = {}\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    texts = []\n",
    "    for line in open('reddit-@{}.json'.format(subreddit), 'r'):\n",
    "        texts.append(json.loads(line))\n",
    "\n",
    "    reddit_text = []\n",
    "    reddit_author = []\n",
    "\n",
    "    for text in texts:\n",
    "        if text[\"_type\"] == \"snscrape.modules.reddit.Comment\":\n",
    "            try:\n",
    "                if detect(text[\"body\"]) == \"en\":\n",
    "                    reddit_text.append(text[\"body\"])\n",
    "                    reddit_author.append(text[\"author\"])\n",
    "            except:\n",
    "                language = \"error\"\n",
    "        \n",
    "        elif text[\"_type\"] == \"snscrape.modules.reddit.Submission\":\n",
    "            try:\n",
    "                if detect(text[\"title\"]) == \"en\":\n",
    "                    reddit_text.append(text[\"title\"])\n",
    "                    reddit_author.append(text[\"author\"])\n",
    "            except:\n",
    "                language = \"error\"\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    data[\"text\"] = reddit_text\n",
    "    data[\"author\"] = reddit_author\n",
    "\n",
    "    data[\"text\"] = data[\"text\"].apply(clean_text)\n",
    "\n",
    "    #aggregate text\n",
    "    agg_text[subreddit] = \" \".join(data[\"text\"].values)\n",
    "\n",
    "    #sentiment analysis\n",
    "    data[\"Positive\"] = data[\"text\"].apply(find_positive)\n",
    "    data[\"Neutral\"] = data[\"text\"].apply(find_neutral)\n",
    "    data[\"Negative\"] = data[\"text\"].apply(find_negative)\n",
    "    data[\"Compound\"] = data[\"text\"].apply(find_compound)\n",
    "\n",
    "    positive_scores[subreddit] = data[\"Positive\"].mean()\n",
    "    neutral_scroes[subreddit] = data[\"Neutral\"].mean()\n",
    "    negative_scores[subreddit] = data[\"Negative\"].mean()\n",
    "    compound_scores[subreddit] = data[\"Compound\"].mean()\n",
    "\n",
    "    print(\"The mean positive sentiment score for {}: {}\".format(subreddit, data[\"Positive\"].mean()))\n",
    "    print(\"The mean negative sentiment score for {}: {}\".format(subreddit, data[\"Negative\"].mean()))\n",
    "    print(\"The mena neutral sentiment score for {}: {}\".format(subreddit, data[\"Neutral\"].mean()))\n",
    "    print(\"The mean compound sentiment score for {}: {}\").format(subreddit, data[\"Compound\"].mean())\n",
    "\n",
    "    #most frequent words\n",
    "    words = \" \".join(data[\"text\"].values)\n",
    "    word_cloud = WordCloud().generate(words)\n",
    "    plt.imshow(word_cloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(subreddit)\n",
    "    plt.show()\n",
    "\n",
    "    freq = FreqDist(words.split(\" \"))\n",
    "    freq.plot(15, cumulative=False, title=subreddit)\n",
    "    plt.show()\n",
    "    print(\"The most frequent 20 words in {}: {}\".format(subreddit, str(list(freq)[:20])))\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "graph1 = nx.Graph()\n",
    "\n",
    "for subreddit1, content1 in agg_text.items():\n",
    "    for subreddit2, content2 in agg_text.items():\n",
    "\n",
    "        sentences = [content1, content2]\n",
    "\n",
    "        model = SentenceTransformer('flax-sentence-embeddings/reddit_single-context_mpnet-base')\n",
    "        embeddings = model.encode(sentences)\n",
    "\n",
    "        similarity = cosine_similarity([embeddings[0]], embeddings[1:])\n",
    "\n",
    "        graph1.add_edge(subreddit1, subreddit2, weight=similarity)\n",
    "\n",
    "nx.draw(graph1, nodelist=list(positive_scores.keys()), node_size=list(positive_scores.values()), with_labels=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15cdab398e30ef71ea6bedff180c52df6afb9223e1d11496cd7e9c4786ac7d6b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
